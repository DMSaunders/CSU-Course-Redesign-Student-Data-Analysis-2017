
= CSU-Course-Redesign-Student-Data-Analysis-2017
:idprefix:
:idseparator: -
:sectanchors:
:sectlinks:
:sectnumlevels: 6
:sectnums:
:icons: font
ifdef::env-github[]
:imagesdir: https://github.com/DMSaunders/files/blob/master/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc: macro
:toclevels: 6
:toc-title: 

My analysis of a survey sent out to students in redesigned courses at the largest university system in the United States, California State University. I've included some general tips :fire: for survey analysis. 

TIP: Tools used: SurveyGizmo, Excel, SPSS, QDAMiner, Tableau

[.right.text-center]
image::vADKt0e0.png[Tiger,200,200]

toc::[]

== Background
I analyzed and helped create surveys to evaluate a system-wide course redesign with technology effort. The Course Redesign with Technology (CRT) Team supports the CSU 2025 Graduation Initiative through redesigning high- demand, high failure-rate courses and improving access to these courses. CRT funds a comprehensive program assisting faculty across the CSU system to redesign their courses for student success. At the end of the term in which the redesigned course is offered, CRT collects student survey data to measure which teaching and learning tools have been used and their perceived impact on student learning, student opinions on various course dimensions, and whether student confidence levels change as a result of completing a redesigned course. 

To participate in the CRT program, CSU faculty members outline their ideas for changes in the courses they teach in a request for proposal process. CRT encourages them to adopt and adapt "proven practices" that faculty in previous cohorts established using narrative ePortfolios about their redesign processes and outcomes. Faculty selected to participate in CRT are funded and attend the Summer Institute where they learn about new technologies, discuss various pedagogical approaches, and develop learning communities focused on a specific discipline or technology. These courses are then offered to students in the fall and spring who are asked to provide feedback about their experiences through a survey. 


== Goals of the Analysis

* Which activities and technologies do students use?
* How frequently do students use them?
* Which do they perceive to be effective?
* Do students rate their course highly across several dimensions?
* Does a studentâ€™s perceived confidence level change by the end of the redesigned course?
* How does re-offering a redesign affect the outcomes?
* How does delivery format affect the outcomes?

== The Data
The CRT team requested the course ID from faculty who delivered a redesigned course in spring 2017. Using this ID, they acquired through an internal system-wide process the emails of most of the target population. They emailed *3607* students a link to the survey and received *610* responses. Each student received a specific single-use URL to ensure that a survey can only be completed once by each student. The course survey was deployed May 1st, before final exams. The survey was active for 40 days. Instructors were asked to encourage their students to participate in the survey, and students were incentivized to complete it by having their names entered in a prize raffle. One hundred fifty-four incomplete submissions were removed, leaving *456* complete responses. We did this to simplify the analysis.

NOTE: See csv files for raw data after removing 154 incomplete submissions. 25 rows are provided as an example, but all columns are included. Separate surveys sent out to quarter and semester students produced the two files. All identifying information has been removed or anonymized including instructor names and classes.

== Data Wrangling in Excel
See the student survey word doc for an explanation of most of the columns. I combined the quarter and semester sheets, creating a column for the variable, to produce a single sheet with 144 columns and 458 rows, then removed test data leftover from testing the survey. A separate version of the survey was sent out to each school so students could select their course from a shorter list. SurveyGizmo spit out a separate column for each university's list of redesigned courses in the sample (less than 10 per). As each student should only have been allowed to select and review one course, I collapsed all these columns into one. Unfortunately, in a previous iteration of this survey, we did not test for this adequately.

CAUTION: Test your survey like a maniac! Try and select everything, and if you are able to select mutually exclusive boxes, like 'agree' and 'disagree', change your survey settings. I see this mistake all the time.

Next I created some new independent variables for the analysis.

[options="header"]
|===
|variable | 

|`Unique_ID`
|additional step for keeping track of rows

|`offer_status`
|pulled from our internal data. 'first-time-offered' for a brand new redesigned course or 're-offered' for subsequent iterations

|`redesign_program`
|internal course redesign classifications. 'proven adopting' for redesigns following an approved model 'promising' for experimental redesigns or 'virtual labs' for redesigns featuring online labwork to increase access to courses requiring labs

|`subject_code`
|broader logical categories I created for the courses: 'Engineering' for IE 2A, 'Computer science' for CS 101, and so on

|===

Most of these changes required sorting the rows, which sometimes causes excel to splice a row, reordering some columns but not others, and corrupting the data. Since I save the file at each step of the process, I was able to frequently compare random samples of my current file with the raw data to check for consistency. If Jon Chou was suddenly female and making completely different comments than in the raw file, then I would retreat to the latest uncorrupted version. And of course I used several random samples at each check. I liked to think the people who bore deep holes into a wheel of cheese to check for consistency.

Getting to the structure of the survey: Programmed question pairs made up the bulk, so we could expect the sample sizes for each follow-up question to be all over the board. I renamed and numbered all the survey questions, with the paired questions as #A and #B. 

image::skip logic.png[image,70%,70%, align=text-center]

Then I moved the all the independent variables together in the front for ease of use (facts about the course and student).

NOTE: It is recommended to put demographic and otherwise sensitive survey questions at the end of a survey so as not to scare off the respondent right away.

* What is the relationship between perceived frequency of use and perceived efficacy?
