
= CSU-Course-Redesign-Student-Data-Analysis-2017
Danielle Saunders <dmariesaunders@gmail.com>

:idprefix:
:idseparator: -
:sectanchors:
:sectlinks:
:sectnumlevels: 6
:sectnums:
:toc: macro
:toclevels: 6
:toc-title:

My analysis of a survey sent out to students in redesigned courses at the largest university system in the United States, California State University.

toc::[]

== Background
I analyzed and helped create surveys to evaluate a system-wide course redesign with technology effort. The Course Redesign with Technology (CRT) Team supports the CSU 2025 Graduation Initiative through redesigning high- demand, high failure-rate courses and improving access to these courses. CRT funds a comprehensive program assisting faculty across the CSU system to redesign their courses for student success. At the end of the term in which the redesigned course is offered, CRT collects student survey data to measure which teaching and learning tools have been used and their perceived impact on student learning, student opinions on various course dimensions, and whether student confidence levels change as a result of completing a redesigned course. 

To participate in the CRT program, CSU faculty members outline their ideas for changes in the courses they teach in a request for proposal process. CRT encourages them to adopt and adapt "proven practices" that faculty in previous cohorts established using narrative ePortfolios about their redesign processes and outcomes. Faculty selected to participate in CRT are funded and attend the Summer Institute where they learn about new technologies, discuss various pedagogical approaches, and develop learning communities focused on a specific discipline or technology. These courses are then offered to students in the fall and spring who are asked to provide feedback about their experiences through a survey. The survey is able to address:

* Which activities and technologies do students use?
* How frequently do students use them?
* Which do they perceive to be effective?
* Do students rate varying dimensions of their course highly?
* Does a studentâ€™s perceived confidence level change by the end of the redesigned course?

Additional questions can be addressed in the data analysis process as follows:

* What is the relationship between perceived frequency of use and perceived efficacy?
* How does re-offering a redesign affect the outcomes?
* How does delivery format affect the outcomes?

## The Data
The CRT team requested the course ID from faculty who delivered a redesigned course in spring 2017. Using this ID, they acquired through an internal system-wide process the emails of most of the target population. They emailed *3607* students a link to the survey and received *610* responses. Each student received a specific single-use URL to ensure that a survey can only be completed once by each student. The course survey was deployed May 1st, before final exams. The survey was active for 40 days. Instructors were asked to encourage their students to participate in the survey, and students were incentivized to complete it by having their names entered in a prize raffle. One hundred fifty-four incomplete submissions were removed, leaving *456* complete responses. We did this to simplify the analysis.

[NOTE]
====
See csv files for raw data. 25 rows are provided as an example, but all columns are included. Separate surveys sent out to quarter and semester students produced the two files which I combined. All identifying information has been removed or anonymized including instructor names and classes.
====
